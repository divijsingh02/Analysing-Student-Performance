{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Analysing-student-performance.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divijsingh02/Analysing-student-performance/blob/main/Analysing_student_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL0Wfjid3m15"
      },
      "source": [
        "Importing libraries and dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbG0Fdburzla"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('StudentsPerformance.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXseIsg130Wd"
      },
      "source": [
        "Encoding the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYSrTxirzlg"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), list(range(5)))], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyuXjHQrzlh"
      },
      "source": [
        "##### Spliting dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWt1K-Mxrzlh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8C82rmH4SNh"
      },
      "source": [
        "Scaling the values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0OAz0xqrzlh"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14RHGuM4VF8"
      },
      "source": [
        "Implementing neural network model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNg4cw9Urzli"
      },
      "source": [
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27HpmTJ12CE0"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51FQRRTQrzli"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDqAJBv4rzli"
      },
      "source": [
        "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mean_squared_error'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1O0Yc84kgw"
      },
      "source": [
        "  Training the model with batch size =1 and epoch = 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0VSnXB5rzlj",
        "outputId": "e0fa2d7d-6bed-4090-c108-02829c69d58f"
      },
      "source": [
        "ann.fit(X_train, y_train, batch_size = 128, epochs = 50 ,validation_split=0.1 )  #8 , 200"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - 4s 95ms/step - loss: 14428.2113 - mean_squared_error: 14428.2113 - val_loss: 4999.4854 - val_mean_squared_error: 4999.4854\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 4970.8186 - mean_squared_error: 4970.8186 - val_loss: 4638.5234 - val_mean_squared_error: 4638.5234\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 3488.2695 - mean_squared_error: 3488.2695 - val_loss: 737.5457 - val_mean_squared_error: 737.5457\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 751.6541 - mean_squared_error: 751.6541 - val_loss: 1242.9326 - val_mean_squared_error: 1242.9326\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 778.4428 - mean_squared_error: 778.4428 - val_loss: 553.6439 - val_mean_squared_error: 553.6439\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 344.3437 - mean_squared_error: 344.3437 - val_loss: 330.5439 - val_mean_squared_error: 330.5439\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 212.7440 - mean_squared_error: 212.7440 - val_loss: 140.6303 - val_mean_squared_error: 140.6303\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 85.9963 - mean_squared_error: 85.9963 - val_loss: 78.7422 - val_mean_squared_error: 78.7422\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 64.9921 - mean_squared_error: 64.9921 - val_loss: 43.3292 - val_mean_squared_error: 43.3292\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 42.2333 - mean_squared_error: 42.2333 - val_loss: 35.5902 - val_mean_squared_error: 35.5902\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 33.0565 - mean_squared_error: 33.0565 - val_loss: 34.3170 - val_mean_squared_error: 34.3170\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 31.5894 - mean_squared_error: 31.5894 - val_loss: 35.5835 - val_mean_squared_error: 35.5835\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 24.4329 - mean_squared_error: 24.4329 - val_loss: 30.3644 - val_mean_squared_error: 30.3644\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 21.3144 - mean_squared_error: 21.3144 - val_loss: 29.2835 - val_mean_squared_error: 29.2835\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 19.9999 - mean_squared_error: 19.9999 - val_loss: 28.3318 - val_mean_squared_error: 28.3318\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 19.5113 - mean_squared_error: 19.5113 - val_loss: 27.6932 - val_mean_squared_error: 27.6932\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 18.1766 - mean_squared_error: 18.1766 - val_loss: 26.3810 - val_mean_squared_error: 26.3810\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 17.5431 - mean_squared_error: 17.5431 - val_loss: 26.4460 - val_mean_squared_error: 26.4460\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 16.5801 - mean_squared_error: 16.5801 - val_loss: 25.8478 - val_mean_squared_error: 25.8478\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 16.4904 - mean_squared_error: 16.4904 - val_loss: 25.1250 - val_mean_squared_error: 25.1250\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 14.9257 - mean_squared_error: 14.9257 - val_loss: 24.8039 - val_mean_squared_error: 24.8039\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 15.9343 - mean_squared_error: 15.9343 - val_loss: 24.5788 - val_mean_squared_error: 24.5788\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 14.8616 - mean_squared_error: 14.8616 - val_loss: 25.0331 - val_mean_squared_error: 25.0331\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 14.0902 - mean_squared_error: 14.0902 - val_loss: 23.9133 - val_mean_squared_error: 23.9133\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 13.7712 - mean_squared_error: 13.7712 - val_loss: 24.1570 - val_mean_squared_error: 24.1570\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 13.9060 - mean_squared_error: 13.9060 - val_loss: 23.5773 - val_mean_squared_error: 23.5773\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 13.2973 - mean_squared_error: 13.2973 - val_loss: 23.5590 - val_mean_squared_error: 23.5590\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 13.7954 - mean_squared_error: 13.7954 - val_loss: 23.2742 - val_mean_squared_error: 23.2742\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 13.0906 - mean_squared_error: 13.0906 - val_loss: 23.1121 - val_mean_squared_error: 23.1121\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 12.2757 - mean_squared_error: 12.2757 - val_loss: 23.5214 - val_mean_squared_error: 23.5214\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.9948 - mean_squared_error: 12.9948 - val_loss: 23.1729 - val_mean_squared_error: 23.1729\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 11.5551 - mean_squared_error: 11.5551 - val_loss: 22.5169 - val_mean_squared_error: 22.5169\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.2925 - mean_squared_error: 12.2925 - val_loss: 22.2878 - val_mean_squared_error: 22.2878\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.6090 - mean_squared_error: 12.6090 - val_loss: 25.5799 - val_mean_squared_error: 25.5799\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 13.2192 - mean_squared_error: 13.2192 - val_loss: 22.5893 - val_mean_squared_error: 22.5893\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 12.8359 - mean_squared_error: 12.8359 - val_loss: 22.6740 - val_mean_squared_error: 22.6740\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 11.5782 - mean_squared_error: 11.5782 - val_loss: 22.5107 - val_mean_squared_error: 22.5107\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 11.7622 - mean_squared_error: 11.7622 - val_loss: 22.1418 - val_mean_squared_error: 22.1418\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.4394 - mean_squared_error: 12.4394 - val_loss: 22.5668 - val_mean_squared_error: 22.5668\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 11.6909 - mean_squared_error: 11.6909 - val_loss: 22.9203 - val_mean_squared_error: 22.9203\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 11.1084 - mean_squared_error: 11.1084 - val_loss: 22.8614 - val_mean_squared_error: 22.8614\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 11.3902 - mean_squared_error: 11.3902 - val_loss: 23.3925 - val_mean_squared_error: 23.3925\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 12.4558 - mean_squared_error: 12.4558 - val_loss: 22.5447 - val_mean_squared_error: 22.5447\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 11.2325 - mean_squared_error: 11.2325 - val_loss: 23.6538 - val_mean_squared_error: 23.6538\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.4412 - mean_squared_error: 12.4412 - val_loss: 22.6493 - val_mean_squared_error: 22.6493\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 12.2946 - mean_squared_error: 12.2946 - val_loss: 22.1996 - val_mean_squared_error: 22.1996\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 11.5314 - mean_squared_error: 11.5314 - val_loss: 22.6042 - val_mean_squared_error: 22.6042\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 11.3366 - mean_squared_error: 11.3366 - val_loss: 23.2801 - val_mean_squared_error: 23.2801\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 10.8425 - mean_squared_error: 10.8425 - val_loss: 22.1326 - val_mean_squared_error: 22.1326\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 10.3637 - mean_squared_error: 10.3637 - val_loss: 23.2571 - val_mean_squared_error: 23.2571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f366006d950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqlfl11R5Iur"
      },
      "source": [
        "Mean squared loss = 2.45 which is quite less hence our model works great\n",
        "It shows that there is a great correlation between reading score and the rest of the features of the data\n",
        "\n",
        "Here , I have compared predicted values for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN71P1krrzlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ad3352-fec7-4d7a-fa55-3e8970160ba4"
      },
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "test_mean_sq_error=0\n",
        "n=len(y_pred)\n",
        "for i in range(n):\n",
        "\n",
        "  if y_pred[i]-int(y_pred[i])>=0.5:\n",
        "    y_pred[i]=int(y_pred[i])+1\n",
        "  else:\n",
        "    y_pred[i]=int(y_pred[i])\n",
        "for i in range(n):\n",
        "  test_mean_sq_error+=abs(y_pred[i]-y_test[i])\n",
        "print(test_mean_sq_error/n)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.355]\n",
            "[[ 55.  59.]\n",
            " [ 75.  72.]\n",
            " [ 76.  77.]\n",
            " [ 60.  64.]\n",
            " [ 59.  52.]\n",
            " [ 55.  56.]\n",
            " [ 54.  50.]\n",
            " [ 42.  31.]\n",
            " [ 69.  71.]\n",
            " [ 83.  84.]\n",
            " [ 62.  66.]\n",
            " [ 57.  55.]\n",
            " [ 67.  76.]\n",
            " [ 76.  80.]\n",
            " [ 75.  72.]\n",
            " [ 84.  81.]\n",
            " [ 91.  86.]\n",
            " [ 95.  95.]\n",
            " [ 62.  60.]\n",
            " [ 56.  48.]\n",
            " [ 88.  84.]\n",
            " [ 82.  86.]\n",
            " [ 74.  77.]\n",
            " [ 58.  61.]\n",
            " [ 53.  59.]\n",
            " [ 65.  69.]\n",
            " [ 72.  76.]\n",
            " [ 72.  62.]\n",
            " [ 68.  66.]\n",
            " [ 73.  70.]\n",
            " [ 50.  42.]\n",
            " [ 76.  76.]\n",
            " [ 51.  55.]\n",
            " [ 70.  64.]\n",
            " [ 63.  59.]\n",
            " [ 82.  85.]\n",
            " [ 68.  74.]\n",
            " [ 58.  59.]\n",
            " [ 63.  65.]\n",
            " [100.  93.]\n",
            " [ 79.  84.]\n",
            " [ 80.  76.]\n",
            " [ 74.  72.]\n",
            " [ 70.  74.]\n",
            " [ 92.  86.]\n",
            " [ 94.  91.]\n",
            " [ 64.  66.]\n",
            " [ 70.  73.]\n",
            " [ 55.  58.]\n",
            " [ 94.  99.]\n",
            " [ 83.  81.]\n",
            " [ 88.  87.]\n",
            " [ 77.  74.]\n",
            " [ 77.  81.]\n",
            " [ 78.  75.]\n",
            " [ 51.  56.]\n",
            " [ 79.  76.]\n",
            " [ 73.  82.]\n",
            " [ 58.  47.]\n",
            " [ 99.  99.]\n",
            " [ 51.  52.]\n",
            " [ 55.  52.]\n",
            " [ 43.  45.]\n",
            " [ 61.  58.]\n",
            " [ 53.  52.]\n",
            " [ 65.  64.]\n",
            " [ 66.  58.]\n",
            " [ 67.  59.]\n",
            " [ 51.  41.]\n",
            " [ 34.  32.]\n",
            " [ 58.  61.]\n",
            " [ 77.  77.]\n",
            " [ 84.  83.]\n",
            " [ 59.  57.]\n",
            " [ 52.  54.]\n",
            " [ 65.  64.]\n",
            " [ 76.  83.]\n",
            " [ 55.  57.]\n",
            " [ 43.  37.]\n",
            " [ 79.  76.]\n",
            " [ 70.  73.]\n",
            " [ 84.  84.]\n",
            " [ 53.  58.]\n",
            " [ 72.  68.]\n",
            " [ 81.  77.]\n",
            " [ 77.  76.]\n",
            " [ 94.  90.]\n",
            " [ 80.  77.]\n",
            " [ 63.  66.]\n",
            " [ 47.  53.]\n",
            " [ 80.  79.]\n",
            " [ 86.  88.]\n",
            " [ 73.  74.]\n",
            " [ 81.  87.]\n",
            " [ 78.  72.]\n",
            " [ 54.  57.]\n",
            " [ 66.  66.]\n",
            " [ 78.  76.]\n",
            " [ 78.  75.]\n",
            " [ 63.  67.]\n",
            " [ 69.  68.]\n",
            " [ 72.  72.]\n",
            " [ 84.  84.]\n",
            " [ 80.  74.]\n",
            " [ 60.  59.]\n",
            " [ 56.  58.]\n",
            " [ 77.  72.]\n",
            " [ 73.  66.]\n",
            " [ 67.  66.]\n",
            " [ 64.  68.]\n",
            " [ 68.  71.]\n",
            " [ 47.  51.]\n",
            " [ 80.  73.]\n",
            " [ 25.  17.]\n",
            " [ 72.  74.]\n",
            " [ 88.  91.]\n",
            " [ 84.  85.]\n",
            " [ 75.  73.]\n",
            " [ 89.  92.]\n",
            " [ 53.  52.]\n",
            " [ 71.  68.]\n",
            " [ 61.  60.]\n",
            " [ 48.  57.]\n",
            " [ 83.  84.]\n",
            " [ 76.  66.]\n",
            " [ 45.  40.]\n",
            " [ 60.  55.]\n",
            " [ 73.  75.]\n",
            " [ 98.  97.]\n",
            " [ 81.  74.]\n",
            " [ 90.  90.]\n",
            " [ 66.  64.]\n",
            " [ 88.  87.]\n",
            " [100.  95.]\n",
            " [ 65.  71.]\n",
            " [ 58.  62.]\n",
            " [ 74.  72.]\n",
            " [ 79.  79.]\n",
            " [ 62.  62.]\n",
            " [ 58.  60.]\n",
            " [ 63.  57.]\n",
            " [ 80.  82.]\n",
            " [ 82.  81.]\n",
            " [ 83.  83.]\n",
            " [ 55.  55.]\n",
            " [ 88.  92.]\n",
            " [ 56.  49.]\n",
            " [100. 100.]\n",
            " [ 39.  39.]\n",
            " [ 51.  48.]\n",
            " [ 69.  71.]\n",
            " [100. 100.]\n",
            " [ 64.  64.]\n",
            " [ 80.  79.]\n",
            " [ 86.  85.]\n",
            " [ 53.  51.]\n",
            " [ 66.  72.]\n",
            " [ 79.  80.]\n",
            " [ 39.  39.]\n",
            " [ 70.  72.]\n",
            " [ 67.  62.]\n",
            " [ 77.  71.]\n",
            " [ 92.  95.]\n",
            " [ 47.  58.]\n",
            " [ 62.  72.]\n",
            " [ 63.  67.]\n",
            " [ 72.  74.]\n",
            " [ 52.  54.]\n",
            " [ 82.  84.]\n",
            " [ 57.  55.]\n",
            " [ 71.  77.]\n",
            " [ 48.  45.]\n",
            " [ 55.  52.]\n",
            " [ 77.  83.]\n",
            " [ 59.  56.]\n",
            " [ 77.  81.]\n",
            " [ 70.  76.]\n",
            " [ 83.  89.]\n",
            " [ 92.  90.]\n",
            " [ 61.  63.]\n",
            " [ 89.  86.]\n",
            " [ 51.  55.]\n",
            " [ 63.  59.]\n",
            " [ 75.  78.]\n",
            " [ 57.  54.]\n",
            " [ 75.  64.]\n",
            " [ 84.  84.]\n",
            " [ 80.  80.]\n",
            " [ 64.  63.]\n",
            " [ 93.  87.]\n",
            " [ 64.  66.]\n",
            " [ 69.  68.]\n",
            " [ 86.  87.]\n",
            " [ 76.  77.]\n",
            " [ 69.  63.]\n",
            " [ 62.  66.]\n",
            " [ 63.  62.]\n",
            " [ 76.  76.]\n",
            " [ 64.  64.]\n",
            " [ 80.  80.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hnoBoNrzlj"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}